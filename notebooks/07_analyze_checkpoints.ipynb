{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "Using virchow could be better since it is already pretrained in histology images. We just need to finetune that.\n",
    "\n",
    "We need 256x256 images.\n",
    "\n",
    "In this notebook I'll analyze what the checkpoint that they give us includes and what we get out of the trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Load checkpoint safely\n",
    "# ---------------------------------------------------------\n",
    "def load_checkpoint(path):\n",
    "    print(f\"üìÇ Loading checkpoint: {path}\")\n",
    "    ckpt = torch.load(path, map_location=\"cpu\")\n",
    "    print(f\"‚úî Loaded successfully.\\n\")\n",
    "    return ckpt\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Print top-level structure\n",
    "# ---------------------------------------------------------\n",
    "def print_top_level(ckpt):\n",
    "    print(\"==== üîç TOP-LEVEL KEYS ====\")\n",
    "    for k in ckpt.keys():\n",
    "        print(\" ‚Ä¢\", k)\n",
    "    print()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Get model_state_dict and categorize keys\n",
    "# ---------------------------------------------------------\n",
    "def analyze_state_dict(ckpt):\n",
    "    sd = ckpt.get(\"model_state_dict\", {})\n",
    "    print(f\"==== üì¶ MODEL STATE DICT ({len(sd)} params) ====\\n\")\n",
    "\n",
    "    categories = defaultdict(list)\n",
    "\n",
    "    for k, v in sd.items():\n",
    "        if k.startswith(\"encoder.\"):\n",
    "            categories[\"encoder\"].append(k)\n",
    "        elif k.startswith(\"decoder.\") or \"head\" in k.lower():\n",
    "            categories[\"decoder\"].append(k)\n",
    "        else:\n",
    "            categories[\"other\"].append(k)\n",
    "\n",
    "    for cat, keys in categories.items():\n",
    "        print(f\"--- {cat.upper()} ({len(keys)} keys) ---\")\n",
    "        for k in keys[:20]:            # print only first 20\n",
    "            print(\"  -\", k)\n",
    "        if len(keys) > 20:\n",
    "            print(f\"  ... and {len(keys)-20} more\")\n",
    "        print()\n",
    "\n",
    "    return categories\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Print parameter shapes\n",
    "# ---------------------------------------------------------\n",
    "def print_shapes(ckpt, filter_prefix=None, max_items=20):\n",
    "    sd = ckpt.get(\"model_state_dict\", {})\n",
    "    print(\"==== üî¢ SHAPES ====\\n\")\n",
    "    count = 0\n",
    "\n",
    "    for k, v in sd.items():\n",
    "        if filter_prefix and not k.startswith(filter_prefix):\n",
    "            continue\n",
    "        print(f\"{k:60} {tuple(v.shape)}\")\n",
    "        count += 1\n",
    "        if count >= max_items:\n",
    "            print(\"... truncated ...\\n\")\n",
    "            break\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. Count total parameters\n",
    "# ---------------------------------------------------------\n",
    "def count_params(ckpt):\n",
    "    sd = ckpt.get(\"model_state_dict\", {})\n",
    "    total = sum(np.prod(v.shape) for v in sd.values())\n",
    "    print(f\"==== üßÆ TOTAL PARAMETERS ====\")\n",
    "    print(f\"Total: {total:,} parameters\\n\")\n",
    "    return total\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. Extract encoder-only weights\n",
    "# ---------------------------------------------------------\n",
    "def extract_encoder(ckpt):\n",
    "    sd = ckpt.get(\"model_state_dict\", {})\n",
    "    encoder_sd = {k.replace(\"encoder.\", \"\"): v for k, v in sd.items() if k.startswith(\"encoder.\")}\n",
    "    print(f\"==== üéØ ENCODER EXTRACT ====\")\n",
    "    print(f\"Extracted {len(encoder_sd)} encoder weights\\n\")\n",
    "    return encoder_sd\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. Compare two checkpoints\n",
    "# ---------------------------------------------------------\n",
    "def compare_checkpoints(ckpt1, ckpt2):\n",
    "    sd1 = ckpt1.get(\"model_state_dict\", {})\n",
    "    sd2 = ckpt2.get(\"model_state_dict\", {})\n",
    "\n",
    "    print(\"==== üîç COMPARISON ====\\n\")\n",
    "\n",
    "    keys1 = set(sd1.keys())\n",
    "    keys2 = set(sd2.keys())\n",
    "\n",
    "    print(\"Missing in ckpt2:\")\n",
    "    for k in sorted(keys1 - keys2):\n",
    "        print(\"  -\", k)\n",
    "    print()\n",
    "\n",
    "    print(\"Missing in ckpt1:\")\n",
    "    for k in sorted(keys2 - keys1):\n",
    "        print(\"  -\", k)\n",
    "    print()\n",
    "\n",
    "    print(\"==== Shape mismatches ====\")\n",
    "    for k in sorted(keys1 & keys2):\n",
    "        if sd1[k].shape != sd2[k].shape:\n",
    "            print(f\"  {k}: {tuple(sd1[k].shape)}  vs  {tuple(sd2[k].shape)}\")\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading checkpoint: /projectnb/ec500kb/projects/Fall_2025_Projects/Project_2/AI-guided-whole-slide-imaging-analysis/CellViT-plus-plus/checkpoints/Virchow/CellViT-Virchow-x40-AMP.pth\n",
      "‚úî Loaded successfully.\n",
      "\n",
      "üìÇ Loading checkpoint: /projectnb/ec500kb/projects/Fall_2025_Projects/Project_2/AI-guided-whole-slide-imaging-analysis/ProcessedDataset/v1_40x_area20/patches_cellvit_p128_pannuke/logs_local/2025-11-15T150541_tcga_finetune_128_reheat/checkpoints/model_best.pth\n",
      "‚úî Loaded successfully.\n",
      "\n",
      "==== üîç TOP-LEVEL KEYS ‚Äî Virchow Official ====\n",
      " ‚Ä¢ arch\n",
      " ‚Ä¢ epoch\n",
      " ‚Ä¢ model_state_dict\n",
      " ‚Ä¢ config\n",
      " ‚Ä¢ scaler_state_dict\n",
      "\n",
      "==== üîç TOP-LEVEL KEYS ‚Äî Your Fine-tuned Model ====\n",
      " ‚Ä¢ arch\n",
      " ‚Ä¢ epoch\n",
      " ‚Ä¢ model_state_dict\n",
      " ‚Ä¢ optimizer_state_dict\n",
      " ‚Ä¢ scheduler_state_dict\n",
      " ‚Ä¢ best_metric\n",
      " ‚Ä¢ best_epoch\n",
      " ‚Ä¢ config\n",
      " ‚Ä¢ wandb_id\n",
      " ‚Ä¢ logdir\n",
      " ‚Ä¢ run_name\n",
      " ‚Ä¢ scaler_state_dict\n",
      "\n",
      "==== üì¶ STATE DICT ANALYSIS (Virchow Official) ‚Äî 743 tensors ====\n",
      "\n",
      "--- ENCODER (456 keys) ---\n",
      "  - encoder.cls_token\n",
      "  - encoder.pos_embed\n",
      "  - encoder.patch_embed.proj.weight\n",
      "  - encoder.patch_embed.proj.bias\n",
      "  - encoder.blocks.0.norm1.weight\n",
      "  - encoder.blocks.0.norm1.bias\n",
      "  - encoder.blocks.0.attn.qkv.weight\n",
      "  - encoder.blocks.0.attn.qkv.bias\n",
      "  - encoder.blocks.0.attn.proj.weight\n",
      "  - encoder.blocks.0.attn.proj.bias\n",
      "  - encoder.blocks.0.ls1.gamma\n",
      "  - encoder.blocks.0.norm2.weight\n",
      "  - encoder.blocks.0.norm2.bias\n",
      "  - encoder.blocks.0.mlp.fc1.weight\n",
      "  - encoder.blocks.0.mlp.fc1.bias\n",
      "  ... +441 more\n",
      "\n",
      "--- DECODER (239 keys) ---\n",
      "  - decoder0.0.block.0.weight\n",
      "  - decoder0.0.block.0.bias\n",
      "  - decoder0.0.block.1.weight\n",
      "  - decoder0.0.block.1.bias\n",
      "  - decoder0.0.block.1.running_mean\n",
      "  - decoder0.0.block.1.running_var\n",
      "  - decoder0.0.block.1.num_batches_tracked\n",
      "  - decoder0.1.block.0.weight\n",
      "  - decoder0.1.block.0.bias\n",
      "  - decoder0.1.block.1.weight\n",
      "  - decoder0.1.block.1.bias\n",
      "  - decoder0.1.block.1.running_mean\n",
      "  - decoder0.1.block.1.running_var\n",
      "  - decoder0.1.block.1.num_batches_tracked\n",
      "  - decoder1.0.block.0.weight\n",
      "  ... +224 more\n",
      "\n",
      "--- HEAD (48 keys) ---\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.0.weight\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.0.bias\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.1.weight\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.1.bias\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.1.running_mean\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.1.running_var\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.1.num_batches_tracked\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.0.weight\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.0.bias\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.1.weight\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.1.bias\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.1.running_mean\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.1.running_var\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.1.num_batches_tracked\n",
      "  - nuclei_binary_map_decoder.decoder0_header.2.weight\n",
      "  ... +33 more\n",
      "\n",
      "==== üì¶ STATE DICT ANALYSIS (Your Model) ‚Äî 746 tensors ====\n",
      "\n",
      "--- ENCODER (457 keys) ---\n",
      "  - encoder.pos_embed\n",
      "  - encoder.patch_embed.proj.weight\n",
      "  - encoder.patch_embed.proj.bias\n",
      "  - encoder.blocks.0.norm1.weight\n",
      "  - encoder.blocks.0.norm1.bias\n",
      "  - encoder.blocks.0.attn.rel_pos_h\n",
      "  - encoder.blocks.0.attn.rel_pos_w\n",
      "  - encoder.blocks.0.attn.qkv.weight\n",
      "  - encoder.blocks.0.attn.qkv.bias\n",
      "  - encoder.blocks.0.attn.proj.weight\n",
      "  - encoder.blocks.0.attn.proj.bias\n",
      "  - encoder.blocks.0.norm2.weight\n",
      "  - encoder.blocks.0.norm2.bias\n",
      "  - encoder.blocks.0.mlp.lin1.weight\n",
      "  - encoder.blocks.0.mlp.lin1.bias\n",
      "  ... +442 more\n",
      "\n",
      "--- DECODER (239 keys) ---\n",
      "  - decoder0.0.block.0.weight\n",
      "  - decoder0.0.block.0.bias\n",
      "  - decoder0.0.block.1.weight\n",
      "  - decoder0.0.block.1.bias\n",
      "  - decoder0.0.block.1.running_mean\n",
      "  - decoder0.0.block.1.running_var\n",
      "  - decoder0.0.block.1.num_batches_tracked\n",
      "  - decoder0.1.block.0.weight\n",
      "  - decoder0.1.block.0.bias\n",
      "  - decoder0.1.block.1.weight\n",
      "  - decoder0.1.block.1.bias\n",
      "  - decoder0.1.block.1.running_mean\n",
      "  - decoder0.1.block.1.running_var\n",
      "  - decoder0.1.block.1.num_batches_tracked\n",
      "  - decoder1.0.block.0.weight\n",
      "  ... +224 more\n",
      "\n",
      "--- HEAD (50 keys) ---\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.0.weight\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.0.bias\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.1.weight\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.1.bias\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.1.running_mean\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.1.running_var\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.1.num_batches_tracked\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.0.weight\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.0.bias\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.1.weight\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.1.bias\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.1.running_mean\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.1.running_var\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.1.num_batches_tracked\n",
      "  - nuclei_binary_map_decoder.decoder0_header.2.weight\n",
      "  ... +35 more\n",
      "\n",
      "==== üßÆ PARAMETER COUNT ‚Äî Virchow Official ====\n",
      "693,982,912.0 parameters.\n",
      "\n",
      "==== üßÆ PARAMETER COUNT ‚Äî Your Model ====\n",
      "699,755,694.0 parameters.\n",
      "\n",
      "==== üî¢ SHAPES (filter: encoder.) ====\n",
      "\n",
      "encoder.cls_token                                            (1, 1, 1280)\n",
      "encoder.pos_embed                                            (1, 257, 1280)\n",
      "encoder.patch_embed.proj.weight                              (1280, 3, 14, 14)\n",
      "encoder.patch_embed.proj.bias                                (1280,)\n",
      "encoder.blocks.0.norm1.weight                                (1280,)\n",
      "encoder.blocks.0.norm1.bias                                  (1280,)\n",
      "encoder.blocks.0.attn.qkv.weight                             (3840, 1280)\n",
      "encoder.blocks.0.attn.qkv.bias                               (3840,)\n",
      "encoder.blocks.0.attn.proj.weight                            (1280, 1280)\n",
      "encoder.blocks.0.attn.proj.bias                              (1280,)\n",
      "encoder.blocks.0.ls1.gamma                                   (1280,)\n",
      "encoder.blocks.0.norm2.weight                                (1280,)\n",
      "encoder.blocks.0.norm2.bias                                  (1280,)\n",
      "encoder.blocks.0.mlp.fc1.weight                              (6832, 1280)\n",
      "encoder.blocks.0.mlp.fc1.bias                                (6832,)\n",
      "encoder.blocks.0.mlp.fc2.weight                              (1280, 3416)\n",
      "encoder.blocks.0.mlp.fc2.bias                                (1280,)\n",
      "encoder.blocks.0.ls2.gamma                                   (1280,)\n",
      "encoder.blocks.1.norm1.weight                                (1280,)\n",
      "encoder.blocks.1.norm1.bias                                  (1280,)\n",
      "... truncated ...\n",
      "\n",
      "==== üî¢ SHAPES (filter: encoder.) ====\n",
      "\n",
      "encoder.pos_embed                                            (1, 64, 64, 1280)\n",
      "encoder.patch_embed.proj.weight                              (1280, 3, 16, 16)\n",
      "encoder.patch_embed.proj.bias                                (1280,)\n",
      "encoder.blocks.0.norm1.weight                                (1280,)\n",
      "encoder.blocks.0.norm1.bias                                  (1280,)\n",
      "encoder.blocks.0.attn.rel_pos_h                              (27, 80)\n",
      "encoder.blocks.0.attn.rel_pos_w                              (27, 80)\n",
      "encoder.blocks.0.attn.qkv.weight                             (3840, 1280)\n",
      "encoder.blocks.0.attn.qkv.bias                               (3840,)\n",
      "encoder.blocks.0.attn.proj.weight                            (1280, 1280)\n",
      "encoder.blocks.0.attn.proj.bias                              (1280,)\n",
      "encoder.blocks.0.norm2.weight                                (1280,)\n",
      "encoder.blocks.0.norm2.bias                                  (1280,)\n",
      "encoder.blocks.0.mlp.lin1.weight                             (5120, 1280)\n",
      "encoder.blocks.0.mlp.lin1.bias                               (5120,)\n",
      "encoder.blocks.0.mlp.lin2.weight                             (1280, 5120)\n",
      "encoder.blocks.0.mlp.lin2.bias                               (1280,)\n",
      "encoder.blocks.1.norm1.weight                                (1280,)\n",
      "encoder.blocks.1.norm1.bias                                  (1280,)\n",
      "encoder.blocks.1.attn.rel_pos_h                              (27, 80)\n",
      "... truncated ...\n",
      "\n",
      "==== üîç COMPARISON ‚Äî Missing Keys ====\n",
      "\n",
      "Missing in checkpoint2 (197):\n",
      "  - encoder.blocks.0.ls1.gamma\n",
      "  - encoder.blocks.0.ls2.gamma\n",
      "  - encoder.blocks.0.mlp.fc1.bias\n",
      "  - encoder.blocks.0.mlp.fc1.weight\n",
      "  - encoder.blocks.0.mlp.fc2.bias\n",
      "  - encoder.blocks.0.mlp.fc2.weight\n",
      "  - encoder.blocks.1.ls1.gamma\n",
      "  - encoder.blocks.1.ls2.gamma\n",
      "  - encoder.blocks.1.mlp.fc1.bias\n",
      "  - encoder.blocks.1.mlp.fc1.weight\n",
      "  - encoder.blocks.1.mlp.fc2.bias\n",
      "  - encoder.blocks.1.mlp.fc2.weight\n",
      "  - encoder.blocks.10.ls1.gamma\n",
      "  - encoder.blocks.10.ls2.gamma\n",
      "  - encoder.blocks.10.mlp.fc1.bias\n",
      "  ... +182\n",
      "\n",
      "Missing in checkpoint1 (200):\n",
      "  - classifier_head.bias\n",
      "  - classifier_head.weight\n",
      "  - encoder.blocks.0.attn.rel_pos_h\n",
      "  - encoder.blocks.0.attn.rel_pos_w\n",
      "  - encoder.blocks.0.mlp.lin1.bias\n",
      "  - encoder.blocks.0.mlp.lin1.weight\n",
      "  - encoder.blocks.0.mlp.lin2.bias\n",
      "  - encoder.blocks.0.mlp.lin2.weight\n",
      "  - encoder.blocks.1.attn.rel_pos_h\n",
      "  - encoder.blocks.1.attn.rel_pos_w\n",
      "  - encoder.blocks.1.mlp.lin1.bias\n",
      "  - encoder.blocks.1.mlp.lin1.weight\n",
      "  - encoder.blocks.1.mlp.lin2.bias\n",
      "  - encoder.blocks.1.mlp.lin2.weight\n",
      "  - encoder.blocks.10.attn.rel_pos_h\n",
      "  ... +185\n",
      "\n",
      "\n",
      "==== üîÑ SHAPE MISMATCHES ====\n",
      "\n",
      "encoder.patch_embed.proj.weight                     (1280, 3, 14, 14) ‚Üí (1280, 3, 16, 16)\n",
      "encoder.pos_embed                                   (1, 257, 1280) ‚Üí (1, 64, 64, 1280)\n",
      "\n",
      "==== üìâ LAYER SIMILARITY (L2 norm) ====\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (752640) must match the size of tensor b (983040) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 207\u001b[0m\n\u001b[1;32m    203\u001b[0m print_shapes(ck2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    205\u001b[0m compare_checkpoints(ck1, ck2)\n\u001b[0;32m--> 207\u001b[0m \u001b[43mcompute_layer_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mck1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mck2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Try comparing pos_embed histograms (often very telling)\u001b[39;00m\n\u001b[1;32m    210\u001b[0m plot_histograms(ck1, ck2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder.pos_embed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 152\u001b[0m, in \u001b[0;36mcompute_layer_similarity\u001b[0;34m(ck1, ck2)\u001b[0m\n\u001b[1;32m    150\u001b[0m     v1 \u001b[38;5;241m=\u001b[39m sd1[k]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    151\u001b[0m     v2 \u001b[38;5;241m=\u001b[39m sd2[k]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m     diff \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mv1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv2\u001b[49m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    153\u001b[0m     diffs\u001b[38;5;241m.\u001b[39mappend((k, diff))\n\u001b[1;32m    155\u001b[0m diffs_sorted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(diffs, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m-\u001b[39mx[\u001b[38;5;241m1\u001b[39m])[:\u001b[38;5;241m30\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (752640) must match the size of tensor b (983040) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 0) Paths\n",
    "# ---------------------------------------------------------\n",
    "ckpt1_path = \"/projectnb/ec500kb/projects/Fall_2025_Projects/Project_2/AI-guided-whole-slide-imaging-analysis/CellViT-plus-plus/checkpoints/Virchow/CellViT-Virchow-x40-AMP.pth\"\n",
    "ckpt2_path = \"/projectnb/ec500kb/projects/Fall_2025_Projects/Project_2/AI-guided-whole-slide-imaging-analysis/ProcessedDataset/v1_40x_area20/patches_cellvit_p128_pannuke/logs_local/2025-11-15T150541_tcga_finetune_128_reheat/checkpoints/model_best.pth\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Load checkpoint safely\n",
    "# ---------------------------------------------------------\n",
    "def load_checkpoint(path):\n",
    "    print(f\"üìÇ Loading checkpoint: {path}\")\n",
    "    ckpt = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    print(f\"‚úî Loaded successfully.\\n\")\n",
    "    return ckpt\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Print top-level structure\n",
    "# ---------------------------------------------------------\n",
    "def print_top_level(ckpt, name=\"Checkpoint\"):\n",
    "    print(f\"==== üîç TOP-LEVEL KEYS ‚Äî {name} ====\")\n",
    "    for k in ckpt.keys():\n",
    "        print(\" ‚Ä¢\", k)\n",
    "    print()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Categorize weights into encoder/decoder/other\n",
    "# ---------------------------------------------------------\n",
    "def analyze_state_dict(ckpt, name=\"\"):\n",
    "    sd = ckpt.get(\"model_state_dict\", {})\n",
    "    print(f\"==== üì¶ STATE DICT ANALYSIS ({name}) ‚Äî {len(sd)} tensors ====\\n\")\n",
    "\n",
    "    categories = defaultdict(list)\n",
    "\n",
    "    for k, v in sd.items():\n",
    "        if k.startswith(\"encoder.\"):\n",
    "            categories[\"encoder\"].append(k)\n",
    "        elif \"head\" in k.lower():\n",
    "            categories[\"head\"].append(k)\n",
    "        elif \"decoder\" in k.lower():\n",
    "            categories[\"decoder\"].append(k)\n",
    "        else:\n",
    "            categories[\"other\"].append(k)\n",
    "\n",
    "    # Print grouped keys\n",
    "    for cat, keys in categories.items():\n",
    "        print(f\"--- {cat.upper()} ({len(keys)} keys) ---\")\n",
    "        for k in keys[:15]:\n",
    "            print(\"  -\", k)\n",
    "        if len(keys) > 15:\n",
    "            print(f\"  ... +{len(keys)-15} more\")\n",
    "        print()\n",
    "\n",
    "    return categories\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) Print parameter shapes\n",
    "# ---------------------------------------------------------\n",
    "def print_shapes(ckpt, prefix=None, max_items=20):\n",
    "    sd = ckpt.get(\"model_state_dict\", {})\n",
    "    print(f\"==== üî¢ SHAPES (filter: {prefix}) ====\\n\")\n",
    "    c = 0\n",
    "    for k, v in sd.items():\n",
    "        if prefix and not k.startswith(prefix):\n",
    "            continue\n",
    "        print(f\"{k:60} {tuple(v.shape)}\")\n",
    "        c += 1\n",
    "        if c >= max_items:\n",
    "            print(\"... truncated ...\\n\")\n",
    "            break\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) Count parameters\n",
    "# ---------------------------------------------------------\n",
    "def count_params(ckpt, name=\"\"):\n",
    "    sd = ckpt.get(\"model_state_dict\", {})\n",
    "    total = sum(np.prod(v.shape) for v in sd.values())\n",
    "    print(f\"==== üßÆ PARAMETER COUNT ‚Äî {name} ====\")\n",
    "    print(f\"{total:,} parameters.\\n\")\n",
    "    return total\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6) Extract encoder-only weights\n",
    "# ---------------------------------------------------------\n",
    "def extract_encoder(ckpt):\n",
    "    sd = ckpt.get(\"model_state_dict\", {})\n",
    "    enc = {k.replace(\"encoder.\", \"\"): v for k, v in sd.items() if k.startswith(\"encoder.\")}\n",
    "    print(f\"üéØ Extracted {len(enc)} encoder weights.\\n\")\n",
    "    return enc\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7) Compare two checkpoints in depth\n",
    "# ---------------------------------------------------------\n",
    "def compare_checkpoints(ck1, ck2):\n",
    "    sd1 = ck1.get(\"model_state_dict\", {})\n",
    "    sd2 = ck2.get(\"model_state_dict\", {})\n",
    "\n",
    "    keys1 = set(sd1.keys())\n",
    "    keys2 = set(sd2.keys())\n",
    "\n",
    "    print(\"==== üîç COMPARISON ‚Äî Missing Keys ====\\n\")\n",
    "\n",
    "    missing_in_2 = sorted(keys1 - keys2)\n",
    "    missing_in_1 = sorted(keys2 - keys1)\n",
    "\n",
    "    print(f\"Missing in checkpoint2 ({len(missing_in_2)}):\")\n",
    "    for k in missing_in_2[:15]:\n",
    "        print(\"  -\", k)\n",
    "    if len(missing_in_2) > 15:\n",
    "        print(f\"  ... +{len(missing_in_2)-15}\\n\")\n",
    "\n",
    "    print(f\"Missing in checkpoint1 ({len(missing_in_1)}):\")\n",
    "    for k in missing_in_1[:15]:\n",
    "        print(\"  -\", k)\n",
    "    if len(missing_in_1) > 15:\n",
    "        print(f\"  ... +{len(missing_in_1)-15}\\n\")\n",
    "\n",
    "    print(\"\\n==== üîÑ SHAPE MISMATCHES ====\\n\")\n",
    "    for k in sorted(keys1 & keys2):\n",
    "        if sd1[k].shape != sd2[k].shape:\n",
    "            print(f\"{k:50}  {tuple(sd1[k].shape)} ‚Üí {tuple(sd2[k].shape)}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8) Weight similarity (L2 difference)\n",
    "# ---------------------------------------------------------\n",
    "def compute_layer_similarity(ck1, ck2):\n",
    "    sd1 = ck1.get(\"model_state_dict\", {})\n",
    "    sd2 = ck2.get(\"model_state_dict\", {})\n",
    "\n",
    "    print(\"==== üìâ LAYER SIMILARITY (L2 norm) ====\\n\")\n",
    "    overlaps = sorted(set(sd1.keys()) & set(sd2.keys()))\n",
    "\n",
    "    diffs = []\n",
    "    for k in overlaps:\n",
    "        v1 = sd1[k].float().view(-1)\n",
    "        v2 = sd2[k].float().view(-1)\n",
    "        diff = torch.norm(v1 - v2).item()\n",
    "        diffs.append((k, diff))\n",
    "\n",
    "    diffs_sorted = sorted(diffs, key=lambda x: -x[1])[:30]\n",
    "\n",
    "    for k, d in diffs_sorted:\n",
    "        print(f\"{k:50}  Œî={d:.4f}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 9) Plot weight histograms of encoder\n",
    "# ---------------------------------------------------------\n",
    "def plot_histograms(ck1, ck2, layer=\"encoder.pos_embed\"):\n",
    "    sd1 = ck1[\"model_state_dict\"]\n",
    "    sd2 = ck2[\"model_state_dict\"]\n",
    "\n",
    "    if layer not in sd1 or layer not in sd2:\n",
    "        print(f\"Layer {layer} not found in both checkpoints.\")\n",
    "        return\n",
    "\n",
    "    w1 = sd1[layer].cpu().numpy().flatten()\n",
    "    w2 = sd2[layer].cpu().numpy().flatten()\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.hist(w1, bins=100, alpha=0.5, label=\"ckpt1\")\n",
    "    plt.hist(w2, bins=100, alpha=0.5, label=\"ckpt2\")\n",
    "    plt.title(f\"Weight Histogram ‚Äî {layer}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "#               ANALYSIS STARTS HERE\n",
    "# =========================================================\n",
    "\n",
    "ck1 = load_checkpoint(ckpt1_path)\n",
    "ck2 = load_checkpoint(ckpt2_path)\n",
    "\n",
    "print_top_level(ck1, \"Virchow Official\")\n",
    "print_top_level(ck2, \"Your Fine-tuned Model\")\n",
    "\n",
    "cats1 = analyze_state_dict(ck1, \"Virchow Official\")\n",
    "cats2 = analyze_state_dict(ck2, \"Your Model\")\n",
    "\n",
    "count_params(ck1, \"Virchow Official\")\n",
    "count_params(ck2, \"Your Model\")\n",
    "\n",
    "print_shapes(ck1, \"encoder.\")\n",
    "print_shapes(ck2, \"encoder.\")\n",
    "\n",
    "compare_checkpoints(ck1, ck2)\n",
    "\n",
    "compute_layer_similarity(ck1, ck2)\n",
    "\n",
    "# Try comparing pos_embed histograms (often very telling)\n",
    "plot_histograms(ck1, ck2, \"encoder.pos_embed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading checkpoint: /projectnb/ec500kb/projects/Fall_2025_Projects/Project_2/AI-guided-whole-slide-imaging-analysis/CellViT-plus-plus/checkpoints/SAM/encoder_only_CellViT-SAM-H-x40-AMP.pth\n",
      "‚úî Loaded successfully.\n",
      "\n",
      "üìÇ Loading checkpoint: /projectnb/ec500kb/projects/Fall_2025_Projects/Project_2/AI-guided-whole-slide-imaging-analysis/ProcessedDataset/v1_40x_area20/patches_cellvit_p128_pannuke/logs_local/2025-11-15T150541_tcga_finetune_128_reheat/checkpoints/model_best.pth\n",
      "‚úî Loaded successfully.\n",
      "\n",
      "==== üîç TOP-LEVEL KEYS ‚Äî Virchow Official ====\n",
      " ‚Ä¢ pos_embed\n",
      " ‚Ä¢ patch_embed.proj.weight\n",
      " ‚Ä¢ patch_embed.proj.bias\n",
      " ‚Ä¢ blocks.0.norm1.weight\n",
      " ‚Ä¢ blocks.0.norm1.bias\n",
      " ‚Ä¢ blocks.0.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.0.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.0.attn.qkv.weight\n",
      " ‚Ä¢ blocks.0.attn.qkv.bias\n",
      " ‚Ä¢ blocks.0.attn.proj.weight\n",
      " ‚Ä¢ blocks.0.attn.proj.bias\n",
      " ‚Ä¢ blocks.0.norm2.weight\n",
      " ‚Ä¢ blocks.0.norm2.bias\n",
      " ‚Ä¢ blocks.0.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.0.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.0.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.0.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.1.norm1.weight\n",
      " ‚Ä¢ blocks.1.norm1.bias\n",
      " ‚Ä¢ blocks.1.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.1.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.1.attn.qkv.weight\n",
      " ‚Ä¢ blocks.1.attn.qkv.bias\n",
      " ‚Ä¢ blocks.1.attn.proj.weight\n",
      " ‚Ä¢ blocks.1.attn.proj.bias\n",
      " ‚Ä¢ blocks.1.norm2.weight\n",
      " ‚Ä¢ blocks.1.norm2.bias\n",
      " ‚Ä¢ blocks.1.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.1.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.1.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.1.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.2.norm1.weight\n",
      " ‚Ä¢ blocks.2.norm1.bias\n",
      " ‚Ä¢ blocks.2.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.2.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.2.attn.qkv.weight\n",
      " ‚Ä¢ blocks.2.attn.qkv.bias\n",
      " ‚Ä¢ blocks.2.attn.proj.weight\n",
      " ‚Ä¢ blocks.2.attn.proj.bias\n",
      " ‚Ä¢ blocks.2.norm2.weight\n",
      " ‚Ä¢ blocks.2.norm2.bias\n",
      " ‚Ä¢ blocks.2.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.2.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.2.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.2.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.3.norm1.weight\n",
      " ‚Ä¢ blocks.3.norm1.bias\n",
      " ‚Ä¢ blocks.3.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.3.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.3.attn.qkv.weight\n",
      " ‚Ä¢ blocks.3.attn.qkv.bias\n",
      " ‚Ä¢ blocks.3.attn.proj.weight\n",
      " ‚Ä¢ blocks.3.attn.proj.bias\n",
      " ‚Ä¢ blocks.3.norm2.weight\n",
      " ‚Ä¢ blocks.3.norm2.bias\n",
      " ‚Ä¢ blocks.3.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.3.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.3.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.3.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.4.norm1.weight\n",
      " ‚Ä¢ blocks.4.norm1.bias\n",
      " ‚Ä¢ blocks.4.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.4.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.4.attn.qkv.weight\n",
      " ‚Ä¢ blocks.4.attn.qkv.bias\n",
      " ‚Ä¢ blocks.4.attn.proj.weight\n",
      " ‚Ä¢ blocks.4.attn.proj.bias\n",
      " ‚Ä¢ blocks.4.norm2.weight\n",
      " ‚Ä¢ blocks.4.norm2.bias\n",
      " ‚Ä¢ blocks.4.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.4.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.4.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.4.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.5.norm1.weight\n",
      " ‚Ä¢ blocks.5.norm1.bias\n",
      " ‚Ä¢ blocks.5.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.5.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.5.attn.qkv.weight\n",
      " ‚Ä¢ blocks.5.attn.qkv.bias\n",
      " ‚Ä¢ blocks.5.attn.proj.weight\n",
      " ‚Ä¢ blocks.5.attn.proj.bias\n",
      " ‚Ä¢ blocks.5.norm2.weight\n",
      " ‚Ä¢ blocks.5.norm2.bias\n",
      " ‚Ä¢ blocks.5.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.5.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.5.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.5.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.6.norm1.weight\n",
      " ‚Ä¢ blocks.6.norm1.bias\n",
      " ‚Ä¢ blocks.6.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.6.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.6.attn.qkv.weight\n",
      " ‚Ä¢ blocks.6.attn.qkv.bias\n",
      " ‚Ä¢ blocks.6.attn.proj.weight\n",
      " ‚Ä¢ blocks.6.attn.proj.bias\n",
      " ‚Ä¢ blocks.6.norm2.weight\n",
      " ‚Ä¢ blocks.6.norm2.bias\n",
      " ‚Ä¢ blocks.6.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.6.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.6.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.6.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.7.norm1.weight\n",
      " ‚Ä¢ blocks.7.norm1.bias\n",
      " ‚Ä¢ blocks.7.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.7.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.7.attn.qkv.weight\n",
      " ‚Ä¢ blocks.7.attn.qkv.bias\n",
      " ‚Ä¢ blocks.7.attn.proj.weight\n",
      " ‚Ä¢ blocks.7.attn.proj.bias\n",
      " ‚Ä¢ blocks.7.norm2.weight\n",
      " ‚Ä¢ blocks.7.norm2.bias\n",
      " ‚Ä¢ blocks.7.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.7.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.7.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.7.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.8.norm1.weight\n",
      " ‚Ä¢ blocks.8.norm1.bias\n",
      " ‚Ä¢ blocks.8.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.8.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.8.attn.qkv.weight\n",
      " ‚Ä¢ blocks.8.attn.qkv.bias\n",
      " ‚Ä¢ blocks.8.attn.proj.weight\n",
      " ‚Ä¢ blocks.8.attn.proj.bias\n",
      " ‚Ä¢ blocks.8.norm2.weight\n",
      " ‚Ä¢ blocks.8.norm2.bias\n",
      " ‚Ä¢ blocks.8.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.8.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.8.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.8.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.9.norm1.weight\n",
      " ‚Ä¢ blocks.9.norm1.bias\n",
      " ‚Ä¢ blocks.9.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.9.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.9.attn.qkv.weight\n",
      " ‚Ä¢ blocks.9.attn.qkv.bias\n",
      " ‚Ä¢ blocks.9.attn.proj.weight\n",
      " ‚Ä¢ blocks.9.attn.proj.bias\n",
      " ‚Ä¢ blocks.9.norm2.weight\n",
      " ‚Ä¢ blocks.9.norm2.bias\n",
      " ‚Ä¢ blocks.9.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.9.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.9.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.9.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.10.norm1.weight\n",
      " ‚Ä¢ blocks.10.norm1.bias\n",
      " ‚Ä¢ blocks.10.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.10.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.10.attn.qkv.weight\n",
      " ‚Ä¢ blocks.10.attn.qkv.bias\n",
      " ‚Ä¢ blocks.10.attn.proj.weight\n",
      " ‚Ä¢ blocks.10.attn.proj.bias\n",
      " ‚Ä¢ blocks.10.norm2.weight\n",
      " ‚Ä¢ blocks.10.norm2.bias\n",
      " ‚Ä¢ blocks.10.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.10.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.10.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.10.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.11.norm1.weight\n",
      " ‚Ä¢ blocks.11.norm1.bias\n",
      " ‚Ä¢ blocks.11.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.11.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.11.attn.qkv.weight\n",
      " ‚Ä¢ blocks.11.attn.qkv.bias\n",
      " ‚Ä¢ blocks.11.attn.proj.weight\n",
      " ‚Ä¢ blocks.11.attn.proj.bias\n",
      " ‚Ä¢ blocks.11.norm2.weight\n",
      " ‚Ä¢ blocks.11.norm2.bias\n",
      " ‚Ä¢ blocks.11.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.11.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.11.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.11.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.12.norm1.weight\n",
      " ‚Ä¢ blocks.12.norm1.bias\n",
      " ‚Ä¢ blocks.12.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.12.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.12.attn.qkv.weight\n",
      " ‚Ä¢ blocks.12.attn.qkv.bias\n",
      " ‚Ä¢ blocks.12.attn.proj.weight\n",
      " ‚Ä¢ blocks.12.attn.proj.bias\n",
      " ‚Ä¢ blocks.12.norm2.weight\n",
      " ‚Ä¢ blocks.12.norm2.bias\n",
      " ‚Ä¢ blocks.12.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.12.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.12.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.12.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.13.norm1.weight\n",
      " ‚Ä¢ blocks.13.norm1.bias\n",
      " ‚Ä¢ blocks.13.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.13.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.13.attn.qkv.weight\n",
      " ‚Ä¢ blocks.13.attn.qkv.bias\n",
      " ‚Ä¢ blocks.13.attn.proj.weight\n",
      " ‚Ä¢ blocks.13.attn.proj.bias\n",
      " ‚Ä¢ blocks.13.norm2.weight\n",
      " ‚Ä¢ blocks.13.norm2.bias\n",
      " ‚Ä¢ blocks.13.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.13.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.13.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.13.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.14.norm1.weight\n",
      " ‚Ä¢ blocks.14.norm1.bias\n",
      " ‚Ä¢ blocks.14.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.14.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.14.attn.qkv.weight\n",
      " ‚Ä¢ blocks.14.attn.qkv.bias\n",
      " ‚Ä¢ blocks.14.attn.proj.weight\n",
      " ‚Ä¢ blocks.14.attn.proj.bias\n",
      " ‚Ä¢ blocks.14.norm2.weight\n",
      " ‚Ä¢ blocks.14.norm2.bias\n",
      " ‚Ä¢ blocks.14.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.14.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.14.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.14.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.15.norm1.weight\n",
      " ‚Ä¢ blocks.15.norm1.bias\n",
      " ‚Ä¢ blocks.15.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.15.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.15.attn.qkv.weight\n",
      " ‚Ä¢ blocks.15.attn.qkv.bias\n",
      " ‚Ä¢ blocks.15.attn.proj.weight\n",
      " ‚Ä¢ blocks.15.attn.proj.bias\n",
      " ‚Ä¢ blocks.15.norm2.weight\n",
      " ‚Ä¢ blocks.15.norm2.bias\n",
      " ‚Ä¢ blocks.15.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.15.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.15.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.15.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.16.norm1.weight\n",
      " ‚Ä¢ blocks.16.norm1.bias\n",
      " ‚Ä¢ blocks.16.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.16.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.16.attn.qkv.weight\n",
      " ‚Ä¢ blocks.16.attn.qkv.bias\n",
      " ‚Ä¢ blocks.16.attn.proj.weight\n",
      " ‚Ä¢ blocks.16.attn.proj.bias\n",
      " ‚Ä¢ blocks.16.norm2.weight\n",
      " ‚Ä¢ blocks.16.norm2.bias\n",
      " ‚Ä¢ blocks.16.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.16.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.16.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.16.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.17.norm1.weight\n",
      " ‚Ä¢ blocks.17.norm1.bias\n",
      " ‚Ä¢ blocks.17.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.17.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.17.attn.qkv.weight\n",
      " ‚Ä¢ blocks.17.attn.qkv.bias\n",
      " ‚Ä¢ blocks.17.attn.proj.weight\n",
      " ‚Ä¢ blocks.17.attn.proj.bias\n",
      " ‚Ä¢ blocks.17.norm2.weight\n",
      " ‚Ä¢ blocks.17.norm2.bias\n",
      " ‚Ä¢ blocks.17.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.17.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.17.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.17.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.18.norm1.weight\n",
      " ‚Ä¢ blocks.18.norm1.bias\n",
      " ‚Ä¢ blocks.18.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.18.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.18.attn.qkv.weight\n",
      " ‚Ä¢ blocks.18.attn.qkv.bias\n",
      " ‚Ä¢ blocks.18.attn.proj.weight\n",
      " ‚Ä¢ blocks.18.attn.proj.bias\n",
      " ‚Ä¢ blocks.18.norm2.weight\n",
      " ‚Ä¢ blocks.18.norm2.bias\n",
      " ‚Ä¢ blocks.18.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.18.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.18.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.18.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.19.norm1.weight\n",
      " ‚Ä¢ blocks.19.norm1.bias\n",
      " ‚Ä¢ blocks.19.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.19.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.19.attn.qkv.weight\n",
      " ‚Ä¢ blocks.19.attn.qkv.bias\n",
      " ‚Ä¢ blocks.19.attn.proj.weight\n",
      " ‚Ä¢ blocks.19.attn.proj.bias\n",
      " ‚Ä¢ blocks.19.norm2.weight\n",
      " ‚Ä¢ blocks.19.norm2.bias\n",
      " ‚Ä¢ blocks.19.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.19.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.19.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.19.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.20.norm1.weight\n",
      " ‚Ä¢ blocks.20.norm1.bias\n",
      " ‚Ä¢ blocks.20.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.20.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.20.attn.qkv.weight\n",
      " ‚Ä¢ blocks.20.attn.qkv.bias\n",
      " ‚Ä¢ blocks.20.attn.proj.weight\n",
      " ‚Ä¢ blocks.20.attn.proj.bias\n",
      " ‚Ä¢ blocks.20.norm2.weight\n",
      " ‚Ä¢ blocks.20.norm2.bias\n",
      " ‚Ä¢ blocks.20.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.20.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.20.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.20.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.21.norm1.weight\n",
      " ‚Ä¢ blocks.21.norm1.bias\n",
      " ‚Ä¢ blocks.21.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.21.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.21.attn.qkv.weight\n",
      " ‚Ä¢ blocks.21.attn.qkv.bias\n",
      " ‚Ä¢ blocks.21.attn.proj.weight\n",
      " ‚Ä¢ blocks.21.attn.proj.bias\n",
      " ‚Ä¢ blocks.21.norm2.weight\n",
      " ‚Ä¢ blocks.21.norm2.bias\n",
      " ‚Ä¢ blocks.21.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.21.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.21.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.21.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.22.norm1.weight\n",
      " ‚Ä¢ blocks.22.norm1.bias\n",
      " ‚Ä¢ blocks.22.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.22.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.22.attn.qkv.weight\n",
      " ‚Ä¢ blocks.22.attn.qkv.bias\n",
      " ‚Ä¢ blocks.22.attn.proj.weight\n",
      " ‚Ä¢ blocks.22.attn.proj.bias\n",
      " ‚Ä¢ blocks.22.norm2.weight\n",
      " ‚Ä¢ blocks.22.norm2.bias\n",
      " ‚Ä¢ blocks.22.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.22.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.22.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.22.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.23.norm1.weight\n",
      " ‚Ä¢ blocks.23.norm1.bias\n",
      " ‚Ä¢ blocks.23.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.23.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.23.attn.qkv.weight\n",
      " ‚Ä¢ blocks.23.attn.qkv.bias\n",
      " ‚Ä¢ blocks.23.attn.proj.weight\n",
      " ‚Ä¢ blocks.23.attn.proj.bias\n",
      " ‚Ä¢ blocks.23.norm2.weight\n",
      " ‚Ä¢ blocks.23.norm2.bias\n",
      " ‚Ä¢ blocks.23.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.23.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.23.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.23.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.24.norm1.weight\n",
      " ‚Ä¢ blocks.24.norm1.bias\n",
      " ‚Ä¢ blocks.24.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.24.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.24.attn.qkv.weight\n",
      " ‚Ä¢ blocks.24.attn.qkv.bias\n",
      " ‚Ä¢ blocks.24.attn.proj.weight\n",
      " ‚Ä¢ blocks.24.attn.proj.bias\n",
      " ‚Ä¢ blocks.24.norm2.weight\n",
      " ‚Ä¢ blocks.24.norm2.bias\n",
      " ‚Ä¢ blocks.24.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.24.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.24.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.24.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.25.norm1.weight\n",
      " ‚Ä¢ blocks.25.norm1.bias\n",
      " ‚Ä¢ blocks.25.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.25.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.25.attn.qkv.weight\n",
      " ‚Ä¢ blocks.25.attn.qkv.bias\n",
      " ‚Ä¢ blocks.25.attn.proj.weight\n",
      " ‚Ä¢ blocks.25.attn.proj.bias\n",
      " ‚Ä¢ blocks.25.norm2.weight\n",
      " ‚Ä¢ blocks.25.norm2.bias\n",
      " ‚Ä¢ blocks.25.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.25.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.25.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.25.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.26.norm1.weight\n",
      " ‚Ä¢ blocks.26.norm1.bias\n",
      " ‚Ä¢ blocks.26.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.26.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.26.attn.qkv.weight\n",
      " ‚Ä¢ blocks.26.attn.qkv.bias\n",
      " ‚Ä¢ blocks.26.attn.proj.weight\n",
      " ‚Ä¢ blocks.26.attn.proj.bias\n",
      " ‚Ä¢ blocks.26.norm2.weight\n",
      " ‚Ä¢ blocks.26.norm2.bias\n",
      " ‚Ä¢ blocks.26.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.26.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.26.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.26.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.27.norm1.weight\n",
      " ‚Ä¢ blocks.27.norm1.bias\n",
      " ‚Ä¢ blocks.27.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.27.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.27.attn.qkv.weight\n",
      " ‚Ä¢ blocks.27.attn.qkv.bias\n",
      " ‚Ä¢ blocks.27.attn.proj.weight\n",
      " ‚Ä¢ blocks.27.attn.proj.bias\n",
      " ‚Ä¢ blocks.27.norm2.weight\n",
      " ‚Ä¢ blocks.27.norm2.bias\n",
      " ‚Ä¢ blocks.27.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.27.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.27.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.27.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.28.norm1.weight\n",
      " ‚Ä¢ blocks.28.norm1.bias\n",
      " ‚Ä¢ blocks.28.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.28.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.28.attn.qkv.weight\n",
      " ‚Ä¢ blocks.28.attn.qkv.bias\n",
      " ‚Ä¢ blocks.28.attn.proj.weight\n",
      " ‚Ä¢ blocks.28.attn.proj.bias\n",
      " ‚Ä¢ blocks.28.norm2.weight\n",
      " ‚Ä¢ blocks.28.norm2.bias\n",
      " ‚Ä¢ blocks.28.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.28.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.28.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.28.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.29.norm1.weight\n",
      " ‚Ä¢ blocks.29.norm1.bias\n",
      " ‚Ä¢ blocks.29.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.29.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.29.attn.qkv.weight\n",
      " ‚Ä¢ blocks.29.attn.qkv.bias\n",
      " ‚Ä¢ blocks.29.attn.proj.weight\n",
      " ‚Ä¢ blocks.29.attn.proj.bias\n",
      " ‚Ä¢ blocks.29.norm2.weight\n",
      " ‚Ä¢ blocks.29.norm2.bias\n",
      " ‚Ä¢ blocks.29.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.29.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.29.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.29.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.30.norm1.weight\n",
      " ‚Ä¢ blocks.30.norm1.bias\n",
      " ‚Ä¢ blocks.30.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.30.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.30.attn.qkv.weight\n",
      " ‚Ä¢ blocks.30.attn.qkv.bias\n",
      " ‚Ä¢ blocks.30.attn.proj.weight\n",
      " ‚Ä¢ blocks.30.attn.proj.bias\n",
      " ‚Ä¢ blocks.30.norm2.weight\n",
      " ‚Ä¢ blocks.30.norm2.bias\n",
      " ‚Ä¢ blocks.30.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.30.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.30.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.30.mlp.lin2.bias\n",
      " ‚Ä¢ blocks.31.norm1.weight\n",
      " ‚Ä¢ blocks.31.norm1.bias\n",
      " ‚Ä¢ blocks.31.attn.rel_pos_h\n",
      " ‚Ä¢ blocks.31.attn.rel_pos_w\n",
      " ‚Ä¢ blocks.31.attn.qkv.weight\n",
      " ‚Ä¢ blocks.31.attn.qkv.bias\n",
      " ‚Ä¢ blocks.31.attn.proj.weight\n",
      " ‚Ä¢ blocks.31.attn.proj.bias\n",
      " ‚Ä¢ blocks.31.norm2.weight\n",
      " ‚Ä¢ blocks.31.norm2.bias\n",
      " ‚Ä¢ blocks.31.mlp.lin1.weight\n",
      " ‚Ä¢ blocks.31.mlp.lin1.bias\n",
      " ‚Ä¢ blocks.31.mlp.lin2.weight\n",
      " ‚Ä¢ blocks.31.mlp.lin2.bias\n",
      " ‚Ä¢ neck.0.weight\n",
      " ‚Ä¢ neck.1.weight\n",
      " ‚Ä¢ neck.1.bias\n",
      " ‚Ä¢ neck.2.weight\n",
      " ‚Ä¢ neck.3.weight\n",
      " ‚Ä¢ neck.3.bias\n",
      "\n",
      "==== üîç TOP-LEVEL KEYS ‚Äî Your Fine-tuned Model ====\n",
      " ‚Ä¢ arch\n",
      " ‚Ä¢ epoch\n",
      " ‚Ä¢ model_state_dict\n",
      " ‚Ä¢ optimizer_state_dict\n",
      " ‚Ä¢ scheduler_state_dict\n",
      " ‚Ä¢ best_metric\n",
      " ‚Ä¢ best_epoch\n",
      " ‚Ä¢ config\n",
      " ‚Ä¢ wandb_id\n",
      " ‚Ä¢ logdir\n",
      " ‚Ä¢ run_name\n",
      " ‚Ä¢ scaler_state_dict\n",
      "\n",
      "==== üì¶ STATE DICT ANALYSIS (Virchow Official) ‚Äî 0 tensors ====\n",
      "\n",
      "==== üì¶ STATE DICT ANALYSIS (Your Model) ‚Äî 746 tensors ====\n",
      "\n",
      "--- ENCODER (457 keys) ---\n",
      "  - encoder.pos_embed\n",
      "  - encoder.patch_embed.proj.weight\n",
      "  - encoder.patch_embed.proj.bias\n",
      "  - encoder.blocks.0.norm1.weight\n",
      "  - encoder.blocks.0.norm1.bias\n",
      "  - encoder.blocks.0.attn.rel_pos_h\n",
      "  - encoder.blocks.0.attn.rel_pos_w\n",
      "  - encoder.blocks.0.attn.qkv.weight\n",
      "  - encoder.blocks.0.attn.qkv.bias\n",
      "  - encoder.blocks.0.attn.proj.weight\n",
      "  - encoder.blocks.0.attn.proj.bias\n",
      "  - encoder.blocks.0.norm2.weight\n",
      "  - encoder.blocks.0.norm2.bias\n",
      "  - encoder.blocks.0.mlp.lin1.weight\n",
      "  - encoder.blocks.0.mlp.lin1.bias\n",
      "  ... +442 more\n",
      "\n",
      "--- DECODER (239 keys) ---\n",
      "  - decoder0.0.block.0.weight\n",
      "  - decoder0.0.block.0.bias\n",
      "  - decoder0.0.block.1.weight\n",
      "  - decoder0.0.block.1.bias\n",
      "  - decoder0.0.block.1.running_mean\n",
      "  - decoder0.0.block.1.running_var\n",
      "  - decoder0.0.block.1.num_batches_tracked\n",
      "  - decoder0.1.block.0.weight\n",
      "  - decoder0.1.block.0.bias\n",
      "  - decoder0.1.block.1.weight\n",
      "  - decoder0.1.block.1.bias\n",
      "  - decoder0.1.block.1.running_mean\n",
      "  - decoder0.1.block.1.running_var\n",
      "  - decoder0.1.block.1.num_batches_tracked\n",
      "  - decoder1.0.block.0.weight\n",
      "  ... +224 more\n",
      "\n",
      "--- HEAD (50 keys) ---\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.0.weight\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.0.bias\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.1.weight\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.1.bias\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.1.running_mean\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.1.running_var\n",
      "  - nuclei_binary_map_decoder.decoder0_header.0.block.1.num_batches_tracked\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.0.weight\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.0.bias\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.1.weight\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.1.bias\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.1.running_mean\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.1.running_var\n",
      "  - nuclei_binary_map_decoder.decoder0_header.1.block.1.num_batches_tracked\n",
      "  - nuclei_binary_map_decoder.decoder0_header.2.weight\n",
      "  ... +35 more\n",
      "\n",
      "==== üßÆ PARAMETER COUNT ‚Äî Virchow Official ====\n",
      "0 parameters.\n",
      "\n",
      "==== üßÆ PARAMETER COUNT ‚Äî Your Model ====\n",
      "699,755,694.0 parameters.\n",
      "\n",
      "==== üî¢ SHAPES (filter: encoder.) ====\n",
      "\n",
      "==== üî¢ SHAPES (filter: encoder.) ====\n",
      "\n",
      "encoder.pos_embed                                            (1, 64, 64, 1280)\n",
      "encoder.patch_embed.proj.weight                              (1280, 3, 16, 16)\n",
      "encoder.patch_embed.proj.bias                                (1280,)\n",
      "encoder.blocks.0.norm1.weight                                (1280,)\n",
      "encoder.blocks.0.norm1.bias                                  (1280,)\n",
      "encoder.blocks.0.attn.rel_pos_h                              (27, 80)\n",
      "encoder.blocks.0.attn.rel_pos_w                              (27, 80)\n",
      "encoder.blocks.0.attn.qkv.weight                             (3840, 1280)\n",
      "encoder.blocks.0.attn.qkv.bias                               (3840,)\n",
      "encoder.blocks.0.attn.proj.weight                            (1280, 1280)\n",
      "encoder.blocks.0.attn.proj.bias                              (1280,)\n",
      "encoder.blocks.0.norm2.weight                                (1280,)\n",
      "encoder.blocks.0.norm2.bias                                  (1280,)\n",
      "encoder.blocks.0.mlp.lin1.weight                             (5120, 1280)\n",
      "encoder.blocks.0.mlp.lin1.bias                               (5120,)\n",
      "encoder.blocks.0.mlp.lin2.weight                             (1280, 5120)\n",
      "encoder.blocks.0.mlp.lin2.bias                               (1280,)\n",
      "encoder.blocks.1.norm1.weight                                (1280,)\n",
      "encoder.blocks.1.norm1.bias                                  (1280,)\n",
      "encoder.blocks.1.attn.rel_pos_h                              (27, 80)\n",
      "... truncated ...\n",
      "\n",
      "==== üîç COMPARISON ‚Äî Missing Keys ====\n",
      "\n",
      "Missing in checkpoint2 (0):\n",
      "Missing in checkpoint1 (746):\n",
      "  - classifier_head.bias\n",
      "  - classifier_head.weight\n",
      "  - decoder0.0.block.0.bias\n",
      "  - decoder0.0.block.0.weight\n",
      "  - decoder0.0.block.1.bias\n",
      "  - decoder0.0.block.1.num_batches_tracked\n",
      "  - decoder0.0.block.1.running_mean\n",
      "  - decoder0.0.block.1.running_var\n",
      "  - decoder0.0.block.1.weight\n",
      "  - decoder0.1.block.0.bias\n",
      "  - decoder0.1.block.0.weight\n",
      "  - decoder0.1.block.1.bias\n",
      "  - decoder0.1.block.1.num_batches_tracked\n",
      "  - decoder0.1.block.1.running_mean\n",
      "  - decoder0.1.block.1.running_var\n",
      "  ... +731\n",
      "\n",
      "\n",
      "==== üîÑ SHAPE MISMATCHES ====\n",
      "\n",
      "\n",
      "==== üìâ LAYER SIMILARITY (L2 norm) ====\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m compute_layer_similarity(ck1, ck2)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Try comparing pos_embed histograms (often very telling)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mplot_histograms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mck1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mck2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder.pos_embed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 167\u001b[0m, in \u001b[0;36mplot_histograms\u001b[0;34m(ck1, ck2, layer)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot_histograms\u001b[39m(ck1, ck2, layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder.pos_embed\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     sd1 \u001b[38;5;241m=\u001b[39m \u001b[43mck1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    168\u001b[0m     sd2 \u001b[38;5;241m=\u001b[39m ck2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sd1 \u001b[38;5;129;01mor\u001b[39;00m layer \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sd2:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_state_dict'"
     ]
    }
   ],
   "source": [
    "ckpt1_path = \"/projectnb/ec500kb/projects/Fall_2025_Projects/Project_2/AI-guided-whole-slide-imaging-analysis/CellViT-plus-plus/checkpoints/SAM/encoder_only_CellViT-SAM-H-x40-AMP.pth\"\n",
    "# =========================================================\n",
    "#               ANALYSIS STARTS HERE\n",
    "# =========================================================\n",
    "\n",
    "ck1 = load_checkpoint(ckpt1_path)\n",
    "ck2 = load_checkpoint(ckpt2_path)\n",
    "\n",
    "print_top_level(ck1, \"Virchow Official\")\n",
    "print_top_level(ck2, \"Your Fine-tuned Model\")\n",
    "\n",
    "cats1 = analyze_state_dict(ck1, \"Virchow Official\")\n",
    "cats2 = analyze_state_dict(ck2, \"Your Model\")\n",
    "\n",
    "count_params(ck1, \"Virchow Official\")\n",
    "count_params(ck2, \"Your Model\")\n",
    "\n",
    "print_shapes(ck1, \"encoder.\")\n",
    "print_shapes(ck2, \"encoder.\")\n",
    "\n",
    "compare_checkpoints(ck1, ck2)\n",
    "\n",
    "compute_layer_similarity(ck1, ck2)\n",
    "\n",
    "# Try comparing pos_embed histograms (often very telling)\n",
    "plot_histograms(ck1, ck2, \"encoder.pos_embed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract encoder keys from checkpoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_encoder_only(root_dir, checkpoint_name):\n",
    "    root = Path(root_dir)\n",
    "    input_path = root / checkpoint_name\n",
    "\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"‚ùå Checkpoint not found:\\n{input_path}\")\n",
    "\n",
    "    ckpt = torch.load(input_path, map_location=\"cpu\")\n",
    "\n",
    "    if \"model_state_dict\" not in ckpt:\n",
    "        raise ValueError(\"‚ùå Checkpoint does not contain model_state_dict!\")\n",
    "\n",
    "    full_sd = ckpt[\"model_state_dict\"]\n",
    "\n",
    "    # ---- Extract encoder.* keys ----\n",
    "    encoder_only = {}\n",
    "    for k, v in full_sd.items():\n",
    "        if k.startswith(\"encoder.\"):\n",
    "            new_k = k.replace(\"encoder.\", \"\")   # SAME FORMAT CellViT expects\n",
    "            encoder_only[new_k] = v\n",
    "\n",
    "    output_path = root / f\"encoder_only_{checkpoint_name}\"\n",
    "\n",
    "    # Save ONLY the pure encoder state dict\n",
    "    torch.save(encoder_only, output_path)\n",
    "\n",
    "    print(\"====================================\")\n",
    "    print(f\"üì¶ Input checkpoint : {input_path}\")\n",
    "    print(f\"üîë Extracted keys   : {len(encoder_only)}\")\n",
    "    print(f\"üíæ Saved encoder to : {output_path}\")\n",
    "    print(\"====================================\")\n",
    "\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "üì¶ Input checkpoint : /projectnb/ec500kb/projects/Fall_2025_Projects/Project_2/AI-guided-whole-slide-imaging-analysis/CellViT-plus-plus/checkpoints/SAM/CellViT-SAM-H-x40-AMP.pth\n",
      "üîë Extracted keys   : 457\n",
      "üíæ Saved encoder to : /projectnb/ec500kb/projects/Fall_2025_Projects/Project_2/AI-guided-whole-slide-imaging-analysis/CellViT-plus-plus/checkpoints/SAM/encoder_only_CellViT-SAM-H-x40-AMP.pth\n",
      "====================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/projectnb/ec500kb/projects/Fall_2025_Projects/Project_2/AI-guided-whole-slide-imaging-analysis/CellViT-plus-plus/checkpoints/SAM/encoder_only_CellViT-SAM-H-x40-AMP.pth')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = \"/projectnb/ec500kb/projects/Fall_2025_Projects/Project_2/AI-guided-whole-slide-imaging-analysis/CellViT-plus-plus/checkpoints/SAM\"\n",
    "checkpoint_name= \"CellViT-SAM-H-x40-AMP.pth\"\n",
    "extract_encoder_only(root_dir, checkpoint_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellvit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
